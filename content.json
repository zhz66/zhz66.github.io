{"meta":{"title":"Step by step up","subtitle":"Road to growth","description":"����һ��С�׽����Ĳ��ͣ����͵���һЩ�������������������Լ�����취��","author":"Peter Zhu","url":"http://yoursite.com","root":"/"},"pages":[{"title":"my_first_page","date":"2019-09-17T11:37:58.000Z","updated":"2020-02-06T08:06:31.866Z","comments":true,"path":"my-first-page/index.html","permalink":"http://yoursite.com/my-first-page/index.html","excerpt":"","text":"*[��֪��дЩʲô]��https://www.cnblogs.com/liugang-vip/p/6337580.html��"}],"posts":[{"title":"什么是脉冲神经网络？","slug":"什么是脉冲神经网络？","date":"2020-03-10T01:49:39.000Z","updated":"2020-03-11T10:51:36.517Z","comments":true,"path":"2020/03/10/什么是脉冲神经网络？/","link":"","permalink":"http://yoursite.com/2020/03/10/什么是脉冲神经网络？/","excerpt":"","text":"引言人工神经网络是对生物神经网络的模拟,传统的人工神经网络,其输入和输出均为模拟量,这些模拟量从生物学角度可以解释为:在一定时间内神经元释放脉冲的频率,即脉冲频率编码[1]。然而,研究表明:生物能够快速响应外界的刺激是因为生物神经网络的信息传递依赖于具体的脉冲时刻[2]。为了使计算机系统更智能，脉冲神经网络应运而生了。它第一次完整地被阐述是在1997年Maass发表的文章中[3]。脉冲神经网络是最符合生物神经规则的人造网络，脉冲神经网络与基于脉冲频率编码信息的传统人工神经网络相比，拥有更强大的计算能力，可以模拟各种神经信号和任意的连续函数，非常适合实现大脑神经信号的处理问题，是进行复杂时空信息处理的有效工具[4]。下面，我将简单的介绍一下什么是脉冲神经网络。 神经学的必要储备知识真实的神经元连接图 一个典型的神经元可分为三个功能不同的部分，称为树突、体细胞和轴突;粗略地说，树突扮演着“输入装置”的角色，它收集来自其他神经元的信号并将其传输到体细胞。soma是“中央处理单元”，它执行一个重要的非线性处理步骤:如果到达soma的总输入超过某个阈值，则生成一个输出信号。输出信号由“输出装置”轴突接收，轴突将信号传递给其他神经元。插图显示了一个神经元动作电位的例子(示意图)。动作电位是持续1-2 ms的短电压脉冲，幅值约为100 mV 两个神经元之间的连接称为突触。让我们假设一个神经元通过突触传递一个信号。发出信号的神经元称为突触前细胞，把接收信号的神经元称为突触后细胞。如图所示：信号从突触前神经元j传递到突触后神经元i。突触用虚线圈表示。右下角的轴突连接着其他神经元。 所述神经元信号由短电脉冲组成，可通过将细电极置于神经元的体细胞上或靠近神经元的体细胞或轴突来观察。脉冲，即所谓的动作电位或峰值，其振幅约为100 mV，持续时间一般为1-2 ms。当动作电位沿轴突传播时，脉冲的形式不变。由单个神经元发出的一系列动作电位被称为“脉冲序列”，即按一定的或不规则的间隔发生的一系列刻板印象事件。 下图证明动作电位是刻板的事件。在最大电压时刻排列的膜电位记录显示，动作电位形态变化不大。 注意：脉冲串中的动作电位通常是分离良好的。即使有非常强的输入，也不可能在第一个峰值期间或之后立即激发第二个峰值。两个峰值之间的最小距离定义了神经元的绝对不应期。绝对不应期之后是一个相对不应期，在这个阶段很难激发动作电位，但也不是不可能。 关于突触突触前神经元的轴突与突触后细胞的树突(或索体)接触的部位是突触。脊椎动物大脑中最常见的突触类型是化学突触。在化学突触中，轴突末端非常靠近突触后神经元，在突触前和突触后细胞膜之间只留下一个很小的间隙。这叫做突触间隙。当一个动作电位到达突触时，它会触发一系列复杂的生化处理步骤，导致神经递质从突触前末端释放到突触间隙。一旦递质分子到达突触后侧，它们就会被突触后细胞膜上的专门受体检测到，并(直接或通过生化信号链)引导到特定通道的开口，使细胞外液中的离子流入细胞离子流入反过来又改变了突触后位置的膜电位。因此，化学信号最终被转换成电反应。突触后神经元对突触前突的电压反应称为突触后电位。 神经动力学基础知识细胞膜电位细胞内电极测量细胞内部和周围环境之间的电位差u(t)，可以记录突触后神经元的电位变化。这种电位差叫做膜电位。当神经元无输入的时候的膜电位叫做静息电位。在到达一个峰值后，电位发生变化，最终衰减回静止电位。如果变化是积极的，突触被认为是兴奋的。如果变化为阴性，则突触受到抑制。 静止时，细胞膜已经有-65 mV左右的强负极化。兴奋性突触的输入减少了膜的负极化，因此被称为去极化。进一步增加膜负极化的输入称为超极化。所谓的去极化就是使细胞膜电位有电位升高的趋势比如由-65mv变化到-55mv。剩下两个名词与之相反。 膜电位表示我们将神经元i的膜电位U关于时间t的历程描述为函数Ui（t），初始时刻Ui（t）=Urest。当t=0时突触前神经元j发出一个脉冲信号，Ui（t）的反应公式如下 u i ( t ) - u rest = : &#x3F5; i &#x2062; j ( t ) . u_{i}(t)-u_{\\rm rest}=:\\epsilon_{ij}(t)\\,. 公式的等号右面被定义为突触后电位PSP（postsynaptic potential）。如果Ui（t）-Urest 正的（负的）我们有兴奋性(抑制性)突触后电位或短EPSP (IPSP)下图描绘了由于神经元j发射信号引起的在神经元i兴奋突触处的情形也就是EPSP情形： 注：突触分为兴奋性和抑制性，要想使神经元激活就需要很多兴奋性的突触被激活，也就是发生EPSP。神经元j=1引起的EPSP的时间进程为 &#x3F5; i &#x2062; 1 &#x2062; ( t - t 1 ( f ) ) \\epsilon_{i1}(t-t_{1}^{(f)}) 下面是神经元i激活关于时间t的过程。 由图b推出Ui（t）关于突触后电位PSP的表示： u i &#x2062; ( t ) = &#x2211; j &#x2211; f &#x3F5; i &#x2062; j &#x2062; ( t - t j ( f ) ) + u rest , u_{i}(t)=\\sum_{j}\\sum_{f}\\epsilon_{ij}(t-t_{j}^{(f)})+u_{\\rm rest}\\,, 另一方面，如果在很短的时间间隔内有太多的输入峰值，线性度就会下降。当膜电位达到一个临界值ϑ\\显示。它的运动轨迹表现出一种与简单的sps求和有很大不同的行为:膜电位表现出一种脉冲样的偏移，振幅约为100 mV。就像图C所示这个短电压脉冲会沿着ii神经元的轴突传递到其他神经元的突触上。脉冲过后，膜电位并没有直接返回到静息电位，通过一个低于静息值的超极化阶段。这种超极化被称为“尖峰后电位”。图C中，在电位冲破阈值的那一段时间就是绝对不应期，在阈值下降至静息电位以下的那一段时间叫相应不应期。 脉冲神经网络就像传统人工神经网络的构成一样，主要元素就是神经元，通过神经元之间的连线构成整个网络。在脉冲神经网络中也一样，不同的是，脉冲神经网络由脉冲神经元和可变化调节标量的权值的突触构成，突触将脉冲神经元连接起来就构成了脉冲神经元网络。SNN与ANN最主要的不同就是使用离散的脉冲信号替代ANN网络中传播的连续的模拟信号。为了在这种网络结构上产生脉冲信号，SNN使用了更加复杂也更加贴近于生物的神经元模型。脉冲神经网络几大要素：1.编码方式：编码，将模拟量转化为脉冲序列，然后作为输入层的输入。相对应的也有解码，即将输出层的序列转换为模拟量。 2.神经元模型：人们提出了多种神经元模型，如脉冲响应模型spike response model (SRM)[5]，Izhikevich神经元模型[6]，leaky integrated-and-fire (LIF)神经元模型[7]等等。目前最受人们喜爱的模型是LIF模型。因为它捕捉到了外部输入的直观特性，即电荷在有明显阈值的漏膜上累积。 3.学习规则：也就是突触权重更新规则。 4.网络结构：脉冲神经网络的结构与传统神经网络类似,可分为静态结构和动态结构. 静态结构是指神经元数量和层数不变,只改变权重等参数,包括多层前馈网络 结构和循环网络结构;动态结构是指网络中神经元数量和连接均可以动态调整,典型的代表是进化脉冲神经网络。 5. 为什么目前构造深度SNN比较难？ 因为由于长脉冲序列的不可微性限制了目前流行的反向传播算法，因此SNN的学习规则是开发多层(深)SNN最具挑战性的部分。 SNN学习机制：监督学习和无监督学习。历史进程：最早出现的是非监督学习算法, 通常是基于 Hebb 规则[8]的学习算法. 随后,更具有生物学特性的STDP(Spike-timing-dependent plasticity) 学习规则[9]成为研究热点。因为,时间脉冲序列不可微，所以神经网络的监督学习算法,如BP算法等,无法直接使用. 这一问题吸引了大量研究者的关注,并提出了诸多面向脉冲神经网络的监督学习算法[10]。根据权重调整规则的不同,大致可分为梯度下降学习算法、监督STDP学习算法和基于脉冲序列卷积的学习算法。 脉冲神经网络应用方面：1.结合无监督学习策略,训练脉冲神经网络无监督的提取数据特征,感知外界环境刺激或者进行聚类分析[11]。2.与传统神经网络类似,使用监督学习方法对数据进行分类或进行时间序列预测[12]。 实际训练中，一些神经元知识的补充：侧抑制机制：在神经生物学中，侧抑制是指被激活的神经元降低其相邻神经元活性的能力。侧向抑制使动作电位从兴奋神经元向相邻神经元的横向扩散受阻。被激活的神经元首先抑制(降低膜电位)同一层的其他神经元。如同一层的神经元A和B，A先激发，那么B的膜电位就被A抑制，B的膜电位就相应的降低了。STDP机制：STDP实际上是大脑用来改变其神经连接(突触)的一种生物学过程。由于大脑无与伦比的学习效率早在几十年前就得到了人们的认可，因此将这一规则引入神经网络训练中。重量的塑造是基于以下规则： 任何有助于突触后神经元放电的突触都应该被强化。它的价值应该增加。 那些不能刺激突触后神经元的突触应该被削弱。它的价值应该降低。下面解释一下这个算法是如何工作的四个神经元通过突触连接到一个神经元上。每一个突触前神经元都以自己的速度放电，相应的突触会发出尖峰信号。突触后神经元的突触强度取决于连接突触的强度。现在，由于输入的突增，突触后神经元的膜电位增加，并在跨过阈值后发出突增。当突触后神经元出现峰值时，我们将监测哪些突触前神经元帮助它放电。这可以通过观察哪些突触前神经元在突触后神经元尖峰之前发出尖峰信号来实现。它们通过增加膜电位来帮助突触后突增，因此相应的突触被加强。突触重量增加的因素与下图中所示的突触后和突触前峰值的时间差成反比。LTP代表长期强化，LTD代表长期抑制，“长期”一词是用来区分在实验中观察到的几毫秒范围内的非常短暂的效应。 References[1] Haykin S S. Neural networks and learning machines[M].Upper Saddle River: Pearson Education, 2009: 115-128.[2] Quiroga R Q, Panzeri S. Principles of neural coding[M].Boca Raton, FL: CRC Press, 2013: 5-30.[3] Maass W. Networks of spiking neurons: The third generation of neural network models[J]. Neural Networks, 1997, 10(9): 1659-1671.[4] Ghosh-Dastidar S，Adeli H． Spiking neural networks［J］． International Journal of Neural Systems，2009，19( 4) : 295 － 308．[5] Jolivet, R., Timothy, J., &amp; Gerstner, W. (2003). The spike response model: A frame\u0002work to predict neuronal spike trains. In Artificial neural networks and neural information processing—ICANN/ICONIP 2003 (pp. 846–853). Springer.[6] Izhikevich, E. M., et al. (2003). Simple model of spiking neurons. IEEE Transactionson Neural Networks, 14(6), 1569–1572.[7]Delorme, A., Gautrais, J., Van Rullen, R., &amp; Thorpe, S. (1999). SpikeNET: A simulator for modeling large networks of integrate and fire neurons. Neurocomputing, 26,989–996.[8]Hebb D O． The Organization of Behavior: A Neuropsy- chological Theory［M］． New York: Wiley，1949．[9] Caporale N，Dan Y． Spike timing-dependent plasticity: A Hebbian learning rule［J］． Annual Ｒeview of Neuro- science，2008，31( 1) : 25 － 46．[10] Kasiński A，Ponulak F． Comparison of supervised learning methods for spike time coding in spiking neural networks ［J］． International Journal of Applied Mathematics and Computer Science，2006，16( 1) : 101 － 113．[11]Rekabdar B, Nicolescu M. Using patterns of firing neurons in spiking neural networks for learning and early recognition of spatio-temporal patterns[J]. Neural Computing and Application, 2017, 25(5): 881-897.[12] Mohemmed A, Schliebs S, Matsuda S. Training spiking neural networks to asoociate spatio-temporal input-output spike patter[J].Neurocomputing, 2013, 107(4): 3-10.","categories":[],"tags":[]},{"title":"cotex-m3 指令学习笔记","slug":"cotex-m3-指令学习笔记(1)","date":"2020-03-08T06:25:49.000Z","updated":"2020-03-08T09:26:36.100Z","comments":true,"path":"2020/03/08/cotex-m3-指令学习笔记(1)/","link":"","permalink":"http://yoursite.com/2020/03/08/cotex-m3-指令学习笔记(1)/","excerpt":"","text":"特殊寄存器概述：特殊寄存器组成 Xpsr说明： 中断组说明： control说明： 通过编程控制寄存器或通过异常来切换操作模式 移动数据指令普通寄存器之间：12MOV R8, R3 MVN R8, R3 （MVN是将R3数据按位取反之后传送给R8） 存储器和寄存器之间：单个数据加载存储：LDR和STR12LDRB Rd, [Rn, #offset] ；LDRH Rd, [Rn, #offset] ；LDR Rd, [Rn, #offset] ；LDRD Rd1,Rd2, [Rn, #offset] STRB Rd, [Rn, #offset] ；STRH Rd, [Rn, #offset] ；STR Rd, [Rn, #offset] ；STRD Rd1,Rd2, [Rn, #offset] 多个数据加载：LDM和STM1234516位命令：LDMIA Rd!,&lt;reg list&gt;；STMIA Rd!,&lt;reg list&gt;32位命令：.W 操作代表32位 Thumb-2指令LDMIA.W Rd(!),&lt;reg list&gt;；LDMDB.W Rd(!),&lt;reg list&gt;；STMIA.W Rd(!),&lt;reg list&gt;；STMDB.W Rd(!),&lt;reg list&gt;；注意： IA是先操作后增加，DB是先减小在操作。 索引前命令和索引后命令示例： 对于R13进行操作：pop和push 对于特殊寄存器进行操作： MRS and MSR。123除非您正在访问APSR，否则只能使用MSR或MRS以特权模式访问其他特殊寄存器MRS R0, PSR ; Read Processor status word into R0 MSR CONTROL, R1 ; Write value of R1 into control register 对于立即数用MOV即可 处理数据指令加减乘除：ADD/ADC,SUB,MUL,DIV 乘除32位指令:SMULL，SMLAL，UMULL，UMLAL 逻辑指令： 移位操作： 跳转指令跳出和调回 跳转注意","categories":[],"tags":[]},{"title":"The road of blog after vicissitudes","slug":"The-road-of-blog-after-vicissitudes","date":"2020-02-25T14:03:48.000Z","updated":"2020-02-25T17:26:15.879Z","comments":true,"path":"2020/02/25/The-road-of-blog-after-vicissitudes/","link":"","permalink":"http://yoursite.com/2020/02/25/The-road-of-blog-after-vicissitudes/","excerpt":"","text":"前言很多同学看到英文的标题就不愿意点进来了，在此解释一下，你们看英文有多难受（类比看到数学就头疼）我在整个建立和使用hexo+github建立博客的时候比你们还难受。此篇博客虽然摘录了其他博客，但是本博主的教程绝对值得一看，可以节省同学们的大量时间。 本文是建立在WINDOS基础上建立的，LINUX系统有些步骤会有些许出入。你们可以专门找LINUX版本的博客，如果没有请发邮件我给我，我去试错。 18说了，下面进入正题，老八秘制小汉堡，便宜实惠还管饱，加油，奥利给！！！！！！！！！！！！ 一、准备工作：你需要做的准备工作有如下几样：安装ssh（WIN10自带），github账号，Github客户端，node.js环境，安装git。首先安装ssh、node.js、gitWIN7安装ssh 1注意一点，在上面教程中，安装默认端口为22的那张图片修改为443，如果没改也不要紧，毕竟这个坑我踩过（明明三个月前22端口还好用，过了个年就完犊子了）。肯定你们都着急去点击安装没仔细看这句话了。 安装node.js安装git 然后进行github准备操作：NO.1，注册github账号并且登录进去，在Github首页右上角头像左侧加号点选择 New repositor。 图片中红色字的意思就是假设你的github账号是ABC，那么仓库名为：ABC.github.io，也就是Owner的名字加上 .github.io 创建好了之后点进去仓库，然后看图1点击红色箭头处，然后找到图2的位置，点击choosetheme看图3，设置完毕后你就可以通过 username.github.io(username为你的用户名访问你的博客了) 下载安装登录你的github账号，然后看图4 二、配置Hexo（命令均在bash里面敲）首先你需要建立一个文件夹，这个文件夹将是你的博客大本营（就是安装HEXO的文件夹）。然后右键选择Git Bash。（如果发现没有请重新安装图4内容）依次输入如下内容并回车。新版node.js自动安装npm，如果提示无npm自行解决吧。。npm install hexo-cli -ghexo init #初始化网站npm installhexo g #生成或 hexo generatehexo s #启动本地服务器 或者 hexo server,这一步之后就可以通过http://localhost:4000 查看了如果你可以查看hexo网页证明你安装成功，如果你遇到了问题core-js@&lt;2.6.8 is no longer maintained.那么点击下面链接点我点我点我相关HEXO命令,等看完最后你在看这个也不迟，心急吃不了热豆腐 安装主题hexo cleangit clone https://github.com/litten/hexo-theme-yilia.git themes/yilia找到根目录_config.yml 文件,打开找到 theme：属性并设置为yiliacd themes/yiliagit pullhexo ghexo s此时查看http://localhost:4000 就有新的主题了，yilia相关配置，这个没怎么配置，不过找到了篇常用的设置，图床部分可以使用它的。yilia，click me 三、配置博客到github上。修改_config.yml文件deploy: type: git repo: git@github.com:ABC/ABC.github.io.git #将ABC换成你自己的用户名即可，两处都要替换 branch: master然后保存退出，在bash里面输入：npm install hexo-deployer-git –save 配置SSHcd ~/.sshssh-keygen -t rsa -C “注册github的邮箱地址” #例如：ssh-keygen -t rsa -C “12345686@qq.com“根据提示操作直到一个奇奇怪怪的形状出现。 然后登陆github官网根据图片操作即可。百度搜索开启查看隐藏文件，最后一张图需要复制的东西在 C:\\Users\\用户名\\ .ssh文件夹下后缀名为.pub的文件中。 然后测试一下，在bash里面输入;ssh -T git@github.com 如果提示timeout，或者22端口不可用，那么就是你没仔细看安装ssh的步骤。不过没关系，还有补救办法。 进入C:\\Users\\用户名\\ .ssh文件夹，然后建立一个config文件，如果不会创建，那么就把文件夹里面已经有的白色文件复制粘贴改个名字。 [看我看我看我！](![5WDV1JNYZZJ@U@GC5RSEHFI.png](https://i.loli.net/2020/02/26/VsKRMNPi6DdEa5q.png)) 邮箱改成注册的github邮箱，为了惩罚你们，自己敲一遍。 最后在bash里面输入：git config –global user.name “随便一个名就行”git config –global user.email “github注册的邮箱” 四、测试进入bash输入hexo n “abc”hexo ghexo d然后浏览器URL打上你的仓库名字即可，例如：ABC.github.io 五、埋一点小坑，如何编写自己的博客？你都看到这了，自己找一找文件尝试改一哈，至于如何编写，放心我还是爱你们的，下面有蛛丝马迹。点我点我点我！，易懂版点我点我点我！，炒鸡易懂版如果你会写博客了，恭喜你，给你一个实用的图床。别点我，我怕疼 附上参考的博客（参考的很多，具体框架和图都是这里盗的，不好意思不写）:https://www.cnblogs.com/jackyroc/p/7681938.html","categories":[],"tags":[]},{"title":"科技论文写作-1","slug":"科技论文写作-1","date":"2020-02-25T13:30:02.000Z","updated":"2020-02-25T13:40:04.780Z","comments":true,"path":"2020/02/25/科技论文写作-1/","link":"","permalink":"http://yoursite.com/2020/02/25/科技论文写作-1/","excerpt":"","text":"论文分类： 学位论文（适合入门打基础看，知网上就行） 期刊论文（省刊和非核心、核心、EI、SCI） 会议论文 （国际会议的论文可以看，曲线救国SCI） 快报 （短小精悍，适合刚火的热点类话题）论文格式：不同论文、不同出版社要求不一致，根据往期的期刊自己掌握即可。一些常识格式：","categories":[],"tags":[]},{"title":"Embeded design class 1","slug":"Embeded-design-class-1","date":"2020-02-24T12:20:12.000Z","updated":"2020-02-25T04:44:56.443Z","comments":true,"path":"2020/02/24/Embeded-design-class-1/","link":"","permalink":"http://yoursite.com/2020/02/24/Embeded-design-class-1/","excerpt":"","text":"嵌入式系统的经典组成 嵌入式处理器，其中ESoC是未来重点的发展芯片 几个常用的嵌入式网站（前三个国内可用）：21IC中国电子网嵌入式开发论坛嵌入式信息-电子工程世界网嵌入式控制技术研究室嵌入式世界网 汽车中的嵌入式系统：六大模块：发动机模块，仪表模块，变速模块，车架模块，转向模块，刹车模块。思考：未来人工智能汽车如何设计？","categories":[],"tags":[]},{"title":"安装Brian2那些事","slug":"安装Brian2那些事","date":"2020-02-23T14:38:40.000Z","updated":"2020-02-23T15:24:39.168Z","comments":true,"path":"2020/02/23/安装Brian2那些事/","link":"","permalink":"http://yoursite.com/2020/02/23/安装Brian2那些事/","excerpt":"","text":"Question1:How to install Brian2? First method: by conda.(由于国内的清华源和中科院源停止更新，所以此方法没试过）。1234567conda install -c conda-forge brian2conda install matplotlib nose ipython jupyterconda install -c brian-team brian2tools``` * Second method: by pip.``` bashpipXXX install (--user) brian2 由于博主安装多个版本pip，例如你的python3.5.X版本的pip是pip3.5， 则上述代码写成：pip3.5 install brian2 Question2:checking other relevant module usually ,you need these module as follows: numpy（科学计算），matplotlib（绘图），cython， scipy（科学计算），norse（单元测试） You can open a terminal and input: pythonXXX import numpy as nm IF： &gt;&gt;&gt;NO Module named numpy You need to install numpy: pipXXX install numpy Especially, when you try to install the module norse you will failed ,Don’t Scared: Click me !!!!!!!!!!!! Or copy down sudo apt install git git clone https://github.com/norse/norse cd norse pipXXX install -e . You must to attention a detail . after e. Question3: How do you deal with the lack of your VMWARE’s store ? approach1：delete somethings，and balabal. approach2 : 打开虚拟机设置，点击硬盘，扩充容量，开机之后进入终端输入以下命令: sudo apt-get install gparted sudo gparted 安装之后你就可以通过gparted扩展分区了，如果你想扩充20G的分区，你需要将 dev/fsda2/下面的swap分区先OFF,右键某一个分区，点击XXX/XX的选项，然后就可以随意滑动分区。 原理就是磁盘从左到右是地址递增的，存储盘块就是连续的地址，你想要多少分区分配多少地址就行。","categories":[],"tags":[]},{"title":"Spiking Notes(1)","slug":"Spiking-Notes-1","date":"2020-02-22T07:48:20.000Z","updated":"2020-02-22T16:56:15.062Z","comments":true,"path":"2020/02/22/Spiking-Notes-1/","link":"","permalink":"http://yoursite.com/2020/02/22/Spiking-Notes-1/","excerpt":"","text":"神经网络模型历史第一代 The first generation is based on McCulloch-Pitts neurons as computational units. These are also referred to as perceptrons or threshold gates.they can only give digital output. In fact they are universal for computations with digital input and output, and every boolean function can be computed by some multilayer perceptron with a single hidden layer.第二代（ANN） The second generation is based on computational units that apply an “activation function” with a continuous set of possible output values to a weighted sum (or polynomial) of the inputs.The second generation can compute certain boolean functions with fewer gates than neural nets from the first generation. In addition, neural nets from the second generation are able to compute functions with analog input and output. Another characteristic feature of this second generation of neural network models is that they support learning algorithms that are based on gradient descent such as backprop.However, at least with regard to fast analog computations by networks of neurons in the cortex, the “firing rate interpretation” itself has become questionable. 第三代（SNN） Many biological neural systems use the timing of single action potentials (or “spikes”) to encode information.A third generation of neuralnetwork models which employ spiking neurons (or “integrate-and-fire neurons”) as computational units. 脉冲神经网络基础知识：1The timing of individual computation steps plays a key role for computations in networks of spiking neurons. 2In the simplest (deterministic) model of a spiking neuron one assumes that a neuron v fires whenever its \"potential\" Pv (which models the electric membrane potential at the \"trigger zone\" of neuron v) reaches a certain threshold ®v. This potential Pv is the sum of so-called excitatory postsynaptic potentials (\"EPSPs\") and inhibitory postsynaptic potentials (\"IPSPs\"), which result from the firing of other neurons u that are connected through a \"synapse\" to neuron v. 3突触前神经元U在s时刻的放电对于t时刻Pv的影响（贡献）由以下算式决定：Wu,v · Qu，v（t-s）。权重Wu，v≥0, Qu,v(t-s)是响应函数。权重Wu，v代表神经元u与v之间突触的效能。 4.历史上第一个正式的SNN模型的要素：有限个数的脉冲神经元集合V；神经元突触集合E（V×V）；一个权重Wu，v ≥0；一个响应函数Qu，v（针对那些属于E并且权重大于0的突触设置的）；一个阈值函数Hv。 5.阈值函数图像: 6.第一个正式的SNN模型中的Pv公式:","categories":[],"tags":[]},{"title":"my_first_article","slug":"my-first-article","date":"2019-09-17T11:36:57.000Z","updated":"2020-02-17T15:41:44.512Z","comments":true,"path":"2019/09/17/my-first-article/","link":"","permalink":"http://yoursite.com/2019/09/17/my-first-article/","excerpt":"","text":"Quick StartCreate a new post1$ hexo new \"My New Post\" More info: 同一个机器安装多个Python3版本，pip共存 Quick StartCreate a new post1$ hexo new \"My New Post\" Quick StartCreate a new post1$ hexo new \"My New Post\"","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2019-09-17T11:20:13.649Z","updated":"2019-09-17T11:20:13.650Z","comments":true,"path":"2019/09/17/hello-world/","link":"","permalink":"http://yoursite.com/2019/09/17/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}